#  读书笔记

# 宽度优先搜索策略
原因：（1）深度优先遍历可能会在深度上过“深”而陷入“黑洞”；    
      （2）重要的网页往往距离种子网页比较近，越深的网页的重要性越低；   
      （3）万维网深度最多17层，但到达某面总存在一条很短的路径，宽度优先遍历会以最快的速度达到这个网页；  
      （4）宽度优先遍历有利于多爬虫的合作抓取，多爬虫合作通常先抓取站内链接，抓取的封闭性很强；
      
      
# 解析HTML网页---Jsoup
Maven中配置：   
      <dependency>   
         <groupId>org.jsoup</gorup>   
         <artifactId>jsoup</artifactId>   
         <version>1.10.3</version>   
      </dependency>   

正则表达式：
    （1）对URL进行过滤，只提取符合特定格式的链接；
    （2）提取网页内容；
HTMLParser：
    （1）文本抽取；
    （2）链接抽取；
    （3）资源抽取；
    （4）链接检查；
    （5）站点检查；
    （6）URL重写；
    （7）广告清除；
    （8）将HTML页面转化成XML页面；
    （9）HTML页面清理；
Rhino是一个由Java实现的JavaScript语言解析引擎，Rhino的主要功能是管理脚本执行时的运行环境

非HTML解析：
　　　1.PDF文件：PDFBox解析PDF文件
         （1）FontBox：处理PDF字体的Java类库
         （2）JempBox处理XMP元数据 的Java类库
      2.Office文档：POI项目
         （1）POI读写Excel、Word、PPT文件
         （2）POI-HSMF读写Outlook
         （3）POI-HDGF读写Visio
         （4）POI-HPBF支持Publisher
      3.其他文件

多媒体内容抽取：
      1.抽取视频内容
      （1）视频内容一般分为四部分：帧、镜头、情节和节目
      （2）关键帧的提取---动态规则策略、基于视觉模型的自适应关键帧提取策略、基于镜头边界系数的关键帧提取策略
         a)基于镜头边界系数的关键帧提取分3个步骤进行：
            -> 设置最大关键帧数M 
            -> 每个镜头的非边界过渡区的第一帧确定为关键帧
              【找镜头边界：基于帧差的镜头边界检测方法、基于模型的镜头边界检测方法、基于学习的镜头边界检测方法】
            -> 使用非极大值抑制法确定镜头边界系数极大值并排序，以实现基于镜头边界系数的关键帧提取
      2.JMF（Java视频处理）： 
      （1）功能
          a）在Java Applet和应用程序中播放贵重物品媒体文件，如AVI、MPEG、WAV等；
          b）可以播放从互联网上下载的媒体流；
          c）可以利用麦克风、摄像机等设备截取音频和视频，并只在成多媒体文件；
          d）处理多媒体文件，转换成文件格式；
          e）向互联网上传音频和视频数据流；
          f）在互联网上播放音频和视频数据；
      （2）组件
          a）数据源，如一个媒体文件
          b）截取设备，如麦克风、摄像机等
          c）播放器---Player，JMF中的接口是Player，将音频/视频数据流作为输入，将数据流输出到音箱或屏幕上
          d）处理器---Processor，Processor接口继承了Player接口，支持Player对象所支持的功能外还可以控制对于输入的多媒体数据流进行何种处理以及通过数据源向其他Player对象或Processor对象输出数据 
          e）数据格式---Format，保存多媒体格式信息
          f）管理器，4种管理器Manager、PackageManager、CaptureDeviceManager、PlugInManager
      3.Sourceforge---org.farng.mp3（Java音频处理）：
      （1）音乐：歌手名+歌曲名等元信息，以MP3文件大体分为三部分：
          ->TAG_V2（ID3V2） 包含了作者、作曲、专辑等信息，长度不固定，扩充ID3V1信息
          ->Frame  一系列的帧，由帧头（MP3的位率、采样率、版本等信息）和数据实体两部分组成 
          ->TAG_V1（ID3V1） 包含作者、作曲、专辑等信息，长度128字节

      
# 解析Json数据---Json
Maven中配置：   
      <dependency>   
         <groupId>com.alibabap</gorup>   
         <artifactId>fastjson</artifactId>   
         <version>1.2.35.3</version>   
      </dependency>   


# 评估页面的重要程度
1.链接的欢迎程度---反向链接（即指向当前URL的链接）的数量和质量决定的，定义为IB(P)；   
2.链接的重要程度---关于URL字符串的函数，仅仅考察字符串本身，比如认为".com"和"home"的URL比".cc"和"map"高，定义为IL(P)；   
3.平均链接的深度---根据上面所分析的宽度优先的原则，计算全站的平均链接深度，然后认为距离种子站点越近的重要性越高，定义为ID(P)；   

则网页的重要性I(P)=X*IB(P)+Y*IL(P)，ID(P)由宽度优先遍历规则保证   


# 爬虫队列
保存爬下来的URL，具备以下几个特点：  
1.能够存储海量数据，当内存容量不够时可以固化在硬盘；     
2.快速存取数据；  
3.支持多线程访问；  

因此采用Hash存储，一般选取URL作为key值，选取MD5压缩算法  


# 爬虫架构
一、应该满足的条件
1.分布式  
2.可伸缩性：能通过增加额外的机器和带宽提高抓取速度  
3.性能和有效性：有效使用系统资源，如CPU、网络带宽、和存储空间  
4.质量：针对大部分网络不能及时出现在用户查询中，所以应该首先抓取重要网页  
5.新鲜性：持续运行  
6.更新：爬虫应该取得已获取网页的新的拷贝，例如网站的回贴功能  
7.可扩展性：为了能够支持新的数据格式和新的抓取协议，爬虫应该设计成模块化的形式  

二、实现异步I/O的框架
1.Mina---借由Java的NIO的反应式实现的模拟前摄式模型；  
2.Grizzly---Web服务器GlassFish的I/O核心；  
3.Netty---NIO客户端服务器框架；  
4.Naga---把普通的Socket和ServerSocket封装成支持NIO的形式；  


# 一致性Hash（Consistent Hash） ---解决因特网中的热点（Hot Spot）问题
1.单调性 --- 如果已经有内容通过哈希分配到了相应的缓存中，而又有新的缓冲加入到系统，哈希的结果应该能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区   
2.平衡性 --- 哈希的结果能够尽可能分布到所有缓冲中去，这样使得所有的缓冲空间都得到利用   
3.分散性 --- 不同终端将相同内容映射到不同缓冲中去，好的哈希算法应该能够尽量避免这种不一致的情况发生  
4.负载 --- （从另一个角度看待分散性问题）对特定的缓冲区，也可能被不同用户映射为不同的内容


# 存储
1、Berkeley DB， 内存数据库，内存数据结构并不适合大规模爬虫的应用
   （1）底层实现B树；   
   （2）key/value结构；  
   （3）布隆过滤器；      
2、Heritrix，成熟的开源爬虫软件     
   （1）封闭Berkeley DB，存放所有待处理的链接，过滤已被抓取的连接(BdbUriUniFilter)；   
   （2） 模块化的设计，各模块由CrawlController类协调，负责整个爬虫任务的开始的结束； 


# Nutch --- 开源Java搜索引擎（全文搜索 + Web爬虫）
1.宽度优先搜索对网页进行抓取，但主要着重【存储】和【爬虫】两过程；
【存储】数据文件有三类：
     （1）Web database(WebDB):只在爬虫过程中使用，存储爬虫抓取的网页之间的链接结构信息---Page和Link。WebDB存储了所抓取网页的链接结构图，在链接结构图中Page为节点，Link为边  
          Page：描述网页的特征信息，包括网页内的链接数目、抓取此网页的时间等相关抓取信息，对网页的重要度评分
          Link：描述两Page之间的链接关系        
     （2）segment：存储一次抓取过程抓到的网页以及这些网页的索引（一次爬行会产生很多个段），爬虫爬行过程会根据WebDB中链接关系按照一定的爬行策略生成每次抓取循环所需要的预取列表，然后通过预取列表的URL抓取网页并建立索引，然后存入段中。段是有时效的，网页被爬虫重新抓取后，先前抓取产生的段就作废了  
     （3）index：爬虫抓取的所有网页的索引，它是将所有segment中的索引合并处理后得到的
【爬虫】首先根据WebDB生成一个特抓取的网页的URL集合（预取列表），下载线程Fetcher类开始根据预取列表进行网页抓取。爬虫根据抓取回来的网页更新WebDB，根据更新后的WebDB生成新的预取列表。  “产生-抓取-更新”循环


# 几个概念区分
Heritrix与Nutch的差异：  
   Nutch 只获取并保存可索引的内容。Heritrix则是照单全收。力求保存页面原貌
   Nutch 可以修剪内容，或者对内容格式进行转换。
   Nutch 保存内容为数据库优化格式便于以后索引；刷新替换旧的内容。而Heritrix 是添加(追加)新的内容。
   Nutch 从命令行运行、控制。Heritrix 有 Web 控制管理界面。
   Nutch 的定制能力不够强，不过现在已经有了一定改进。Heritrix 可控制的参数更多。
   Heritrix提供的功能没有nutch多，有点整站下载的味道。既没有索引又没有解析，甚至对于重复爬取URL都处理不是很好。
   Heritrix的功能强大 但是配置起来却有点麻烦。

Heritrix与Nutch的比较：
   1.简单易懂、容易修改；
   2.Nutch支持分布式爬虫，最新版本底层实现使用Hadoop
   一、从功能方面来说，Heritrix与Larbin的功能类似。都是一个纯粹的网络爬虫，提供网站的镜像下载。而Nutch是一个网络搜索引擎框架，爬取网页只是其功能的一部分。
   二、从分布式处理来说，Nutch支持分布式处理，而另外两个（Labin和Heritrix）好像尚且还没有支持。
   三、从爬取的网页存储方式来说，Heritrix和 Larbin都是将爬取下来的内容保存为原始类型的内容。而Nutch是将内容保存到其特定格式的segment中去。
   四，对于爬取下来的内容的处理来说，Heritrix和 Larbin都是将爬取下来的内容不经处理直接保存为原始内容。而Nutch对文本进行了包括链接分析、正文提取、建立索引（Lucene索引）等处理。
   五，从爬取的效率来说，Larbin效率较高，因为其是使用c++实现的并且功能单一。

网络爬虫、分布式数据库、搜索引擎之间的关系：
Nutch+Hadoop：网络爬虫架构，分布式离线批量处理架构，有非常优异的吞吐量和抓取性能并提供了大量的配置定制选项  
ElasticSearch：搜索引擎架构，典型的分布式在线实时交互查询架构，无单点故障，高伸缩、高可用  
Hbase+Hadoop：分布式数据库架构，典型的分布式在线实时随机读写架构。极强的水平伸缩性，支持数十亿的行和数百万的列，能够对网络爬虫提交的数据进行实时写入，并能配合搜索引擎，根据搜索结果实时获取数据

1.网络爬虫将抓取到的HTML页面解析完成之后，把解析出的数据加入缓冲区队列，由其他两个线程负责处理数据，一个线程负责将数据保存到分布式数据库，一个线程负责将数据提交到搜索引擎进行索引  
2.搜索引擎处理用户的搜索条件，并将搜索结果返回给用户，如果用户查看网页快照，则从分布式数据库中获取网页的原始内容  
爬虫集群、分布式数据库集群、搜索引擎集群在物理部署上，可以部署到同一个硬件集群上，也可以分开部署，形成1-3个硬件集群


# 爬虫黑洞
情况：（1）链接本身是一个无限循环；（2）不同URL指向同一张网页；
解决办法：回避动态网页---URL中含问号就是动态网页

# 限定爬虫和主题爬虫
一、主题爬虫：（1）有明确的搜索目标，直接分析源代码结构，获得自己需要的信息，存入数据库；
             （2）根据得到的网页的内容是否包含与主题相关包含的导向词，给每个导向词设置一个权重，就可以优先访问和主题相关的URL【导向词的设置：人工经验手工设置、给定主题相关的网页集合由程序提取共同特征】；
             （3）针对网页链接进行评分；
             （4）链接描述文本分析；
二、限定爬虫：对爬取的主机的范围做出一些限制，如域名、爬取层数、IP、语言等的限制
三、网络道德：侵犯网站隐私的不可抓取
    （1）网站根目录下放置robots.txt，其中规定了哪些内容不想被抓取；
    （2）设置Robots Meta标签


# 网页去“噪声”
一、信息检索领域中评价Web检索系统的两个指标
   （1）检索结果的相关性
   （2）检索的速度 
二、缺少训练集时可以自定义规则来分类，在有训练集的情况下采用支持向量机进行两类分类
   （1）网页相似度计算：将网页本身抽象成字符串或者DOM树，然后求串的最长公共子序列或者树的相似度
   （2）对网页进行分类可以利用的特征有：链接密度、背景颜色、高宽比、在页面位置、平均兔子长度、不在锚点上的文字 
   （3）统计学去噪：删除噪音块->划分段落->评估段落
   （4）根据链接文字比率删除无效节点过程如下：
      a）计算节点下链接数
      b）计算节点下文字数
      c）计算节点的链接文字比 = 节点下链接数/节点下的文字数
      d）如果节点的链接文字比大于某一个阈值，则删除这个节点
   （5） 利用“视觉”消除“噪声” ---基于视觉的网页分块算法VIPS
      Cobra提供了一个纯Java实现的浏览器


# 去除重复文档
1.利用“语义指纹”排重
  自查重，在给定的文档集合内部检测---比较文档的语义缩略，即指纹
2.SimHash排重  


# PageRank算法
 从其他网页链接到一个网页的数量越多，这个网页就越重要（入链数，人气值 ）；
 原理：网页的重要性依赖于它的入链数，如果高等级的文件链接到网页，那网页的等级也高；

 =>爬虫爬取链接时保存网页的出度，并以此来更新对应链接的入度

# HITS算法
 =>ARC算法


 # 网页分类
 特征选择：CHI、信息增益、XGboost
 网页分类：支持向量机、AdaBoost
 网页聚类：DBScan、

